{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e247ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ User Nada authenticated!\n",
      "Draw mode: ON\n",
      "Draw mode: OFF\n",
      "OCR Mode: ON\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pyautogui\n",
    "import time\n",
    "import pyttsx3\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Set path to Tesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Load gesture recognition model\n",
    "with open(r\"D:\\nada mossad\\e-just\\4th grade\\second term\\computer vision\\Project\\gestures\\gesture_rf_model.pkl\", 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {\n",
    "    0: 'palm - play/pause',\n",
    "    1: 'like - volume up',\n",
    "    2: 'dislike - volume down',\n",
    "    3: 'peace - scroll up',\n",
    "    4: 'four - scroll down'\n",
    "}\n",
    "\n",
    "# Face authentication\n",
    "authenticated = False\n",
    "authenticated_user = \"\"\n",
    "\n",
    "# Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.6)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Camera setup\n",
    "cap = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Drawing and gesture variables\n",
    "draw_mode = False\n",
    "canvas = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "prev_x, prev_y = None, None\n",
    "draw_color = (0, 255, 0)\n",
    "prev_gesture = None\n",
    "last_action_time = 0\n",
    "cooldown = 1.5\n",
    "\n",
    "# OCR variables\n",
    "ocr_mode = False\n",
    "last_ocr_text = \"\"\n",
    "ocr_cooldown = 2\n",
    "last_ocr_time = 0\n",
    "\n",
    "# Text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def speak(text):\n",
    "    try:\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "    except Exception as e:\n",
    "        print(f\"[TTS ERROR]: {e}\")\n",
    "\n",
    "def authenticate_user(frame, faces):\n",
    "    global authenticated, authenticated_user\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "        if face_roi.size == 0:\n",
    "            continue\n",
    "\n",
    "        cv2.imwrite('current_face.jpg', face_roi)\n",
    "        try:\n",
    "            user_authenticated = False\n",
    "            for user_image in os.listdir(\"users/\"):  # Assuming user images are stored in \"users\" folder\n",
    "                user_name = os.path.splitext(user_image)[0]  # User name is the filename without extension\n",
    "                user_image_path = os.path.join(\"users\", user_image)\n",
    "\n",
    "                result = DeepFace.verify('current_face.jpg', user_image_path, enforce_detection=False)\n",
    "                if result['verified']:\n",
    "                    authenticated = True\n",
    "                    authenticated_user = user_name\n",
    "                    print(f\"✅ User {authenticated_user} authenticated!\")\n",
    "                    speak(f\"Welcome {authenticated_user}\")\n",
    "                    user_authenticated = True\n",
    "                    break\n",
    "\n",
    "            if not user_authenticated:\n",
    "                print(\"❌ Unknown User\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Face Verification Error]: {e}\")\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "def handle_gesture(gesture):\n",
    "    global last_action_time, prev_gesture\n",
    "\n",
    "    current_time = time.time()\n",
    "    if gesture != prev_gesture or current_time - last_action_time > cooldown:\n",
    "        prev_gesture = gesture\n",
    "        last_action_time = current_time\n",
    "        speak(gesture)\n",
    "\n",
    "        if gesture == 'palm - play/pause':\n",
    "            pyautogui.press('playpause')\n",
    "        elif gesture == 'like - volume up':\n",
    "            pyautogui.press('volumeup')\n",
    "        elif gesture == 'dislike - volume down':\n",
    "            pyautogui.press('volumedown')\n",
    "        elif gesture == 'peace - scroll up':\n",
    "            pyautogui.scroll(300)\n",
    "        elif gesture == 'four - scroll down':\n",
    "            pyautogui.scroll(-300)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"[ERROR] Could not read frame from camera.\")\n",
    "        break\n",
    "\n",
    "    frame_raw = frame.copy()  # Save non-flipped frame for OCR\n",
    "    frame = cv2.flip(frame, 1)  # Flip for mirrored interaction\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(100, 100))\n",
    "\n",
    "    if not authenticated:\n",
    "        authenticate_user(frame, faces)\n",
    "    else:\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks and not ocr_mode:  # ❗ Disable gesture recognition during OCR\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                landmarks = [coord for lm in hand_landmarks.landmark for coord in (lm.x, lm.y)]\n",
    "                h, w, _ = frame.shape\n",
    "                index_tip = hand_landmarks.landmark[8]\n",
    "                cx, cy = int(index_tip.x * w), int(index_tip.y * h)\n",
    "\n",
    "                if draw_mode:\n",
    "                    if prev_x is not None:\n",
    "                        cv2.line(canvas, (prev_x, prev_y), (cx, cy), draw_color, 5)\n",
    "                    prev_x, prev_y = cx, cy\n",
    "                else:\n",
    "                    prev_x, prev_y = None, None\n",
    "                    try:\n",
    "                        prediction = clf.predict(np.array(landmarks).reshape(1, -1))\n",
    "                        gesture = label_mapping[prediction[0]]\n",
    "                        handle_gesture(gesture)\n",
    "                        cv2.putText(frame, f'Gesture: {gesture}', (10, 30),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Prediction Error]: {e}\")\n",
    "\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if ocr_mode:\n",
    "            current_time = time.time()\n",
    "            if current_time - last_ocr_time > ocr_cooldown:\n",
    "                ocr_input = frame_raw.copy()\n",
    "                frame = cv2.resize(ocr_input, (640, 480))\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "                _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "                custom_config = r'--oem 3 --psm 6' \n",
    "                details = pytesseract.image_to_data(thresh, config=custom_config, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "                detected_text = \"\"\n",
    "                for i in range(len(details['text'])):\n",
    "                    text = details['text'][i]\n",
    "                    confidence = int(details['conf'][i]) \n",
    "                    \n",
    "                    if text.strip() != \"\" and confidence > 90:\n",
    "                        detected_text += text + \" \"\n",
    "                        speak(detected_text)\n",
    "                cv2.putText(frame, detected_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        frame = cv2.addWeighted(frame, 1, canvas, 0.5, 0)\n",
    "\n",
    "    # Display frame\n",
    "    cv2.imshow(\"Smart Vision Assistant\", frame)\n",
    "\n",
    "    # Keyboard shortcuts\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    elif key == ord('d'):\n",
    "        draw_mode = not draw_mode\n",
    "        print(\"Draw mode:\", \"ON\" if draw_mode else \"OFF\")\n",
    "    elif key == ord('c'):\n",
    "        canvas.fill(0)\n",
    "        print(\"Canvas cleared\")\n",
    "    elif key == ord('o'):\n",
    "        ocr_mode = not ocr_mode\n",
    "        print(\"OCR Mode:\", \"ON\" if ocr_mode else \"OFF\")\n",
    "    elif key in [ord('r'), ord('g'), ord('b'), ord('y'), ord('k')]:\n",
    "        colors = {'r': (0, 0, 255), 'g': (0, 255, 0), 'b': (255, 0, 0),\n",
    "                  'y': (0, 255, 255), 'k': (0, 0, 0)}\n",
    "        draw_color = colors[chr(key)]\n",
    "        print(f\"Color changed to {chr(key).upper()}\")\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc44de",
   "metadata": {},
   "source": [
    "GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c769123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\mediapipe_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pyautogui\n",
    "import time\n",
    "import pyttsx3\n",
    "from PIL import Image, ImageTk\n",
    "import pytesseract\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import threading\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Set path to Tesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Load gesture recognition model\n",
    "with open(r\"D:\\nada mossad\\e-just\\4th grade\\second term\\computer vision\\Project\\gestures\\gesture_rf_model.pkl\", 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'palm - play/pause',\n",
    "    1: 'like - volume up',\n",
    "    2: 'dislike - volume down',\n",
    "    3: 'peace - scroll up',\n",
    "    4: 'four - scroll down'\n",
    "}\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.6)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Voice engine\n",
    "engine = pyttsx3.init()\n",
    "def speak(text):\n",
    "    try:\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Voice thread control\n",
    "def voice_command_listener():\n",
    "    global voice_thread_running\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "    speak(\"Voice command module is now active.\")\n",
    "    while voice_thread_running:\n",
    "        try:\n",
    "            with mic as source:\n",
    "                print(\"Listening for voice commands...\")\n",
    "                audio = recognizer.listen(source, timeout=5)\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            print(f\"You said: {command}\")\n",
    "            if \"increase brightness\" in command:\n",
    "                pyautogui.press(\"brightnessup\")\n",
    "                speak(\"Increasing brightness.\")\n",
    "            elif \"decrease brightness\" in command:\n",
    "                pyautogui.press(\"brightnessdown\")\n",
    "                speak(\"Decreasing brightness.\")\n",
    "        except sr.WaitTimeoutError:\n",
    "            continue\n",
    "        except sr.UnknownValueError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Voice error: {e}\")\n",
    "    speak(\"Voice command stopped.\")\n",
    "\n",
    "voice_thread_running = False\n",
    "voice_thread = None\n",
    "\n",
    "def toggle_voice_commands():\n",
    "    global voice_thread_running, voice_thread\n",
    "    if not voice_thread_running:\n",
    "        voice_thread_running = True\n",
    "        voice_thread = threading.Thread(target=voice_command_listener)\n",
    "        voice_thread.daemon = True\n",
    "        voice_thread.start()\n",
    "        speak(\"Voice control started\")\n",
    "    else:\n",
    "        voice_thread_running = False\n",
    "        speak(\"Voice control stopped\")\n",
    "\n",
    "# Globals\n",
    "cap = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "authenticated = False\n",
    "authenticated_user = \"\"\n",
    "\n",
    "canvas = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "draw_mode = False\n",
    "ocr_mode = False\n",
    "draw_color = (0, 255, 0)\n",
    "prev_x, prev_y = None, None\n",
    "prev_gesture = None\n",
    "last_action_time = 0\n",
    "cooldown = 1.5\n",
    "ocr_cooldown = 2\n",
    "last_ocr_time = 0\n",
    "\n",
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Smart Vision Assistant\")\n",
    "root.configure(bg=\"#2e2e2e\")\n",
    "\n",
    "style = ttk.Style()\n",
    "style.theme_use(\"clam\")\n",
    "style.configure(\"TButton\", font=(\"Segoe UI\", 10), padding=6, relief=\"flat\", background=\"#3a3a3a\", foreground=\"white\")\n",
    "style.configure(\"TFrame\", background=\"#2e2e2e\")\n",
    "\n",
    "video_label = tk.Label(root)\n",
    "video_label.pack(pady=10)\n",
    "\n",
    "# Functions\n",
    "def toggle_draw_mode():\n",
    "    global draw_mode\n",
    "    draw_mode = not draw_mode\n",
    "    print(\"Draw mode:\", draw_mode)\n",
    "\n",
    "def toggle_ocr_mode():\n",
    "    global ocr_mode\n",
    "    ocr_mode = not ocr_mode\n",
    "    print(\"OCR mode:\", ocr_mode)\n",
    "\n",
    "def clear_canvas():\n",
    "    global canvas\n",
    "    canvas.fill(0)\n",
    "    print(\"Canvas cleared\")\n",
    "\n",
    "def set_color(color):\n",
    "    global draw_color\n",
    "    draw_color = color\n",
    "    print(\"Color changed:\", color)\n",
    "\n",
    "def handle_gesture(gesture):\n",
    "    global prev_gesture, last_action_time\n",
    "    current_time = time.time()\n",
    "    if gesture != prev_gesture or current_time - last_action_time > cooldown:\n",
    "        prev_gesture = gesture\n",
    "        last_action_time = current_time\n",
    "        speak(gesture)\n",
    "\n",
    "        if gesture == 'palm - play/pause':\n",
    "            pyautogui.press('playpause')\n",
    "        elif gesture == 'like - volume up':\n",
    "            pyautogui.press('volumeup')\n",
    "        elif gesture == 'dislike - volume down':\n",
    "            pyautogui.press('volumedown')\n",
    "        elif gesture == 'peace - scroll up':\n",
    "            pyautogui.scroll(300)\n",
    "        elif gesture == 'four - scroll down':\n",
    "            pyautogui.scroll(-300)\n",
    "\n",
    "def authenticate_user(frame, faces):\n",
    "    global authenticated, authenticated_user\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "        if face_roi.size == 0:\n",
    "            continue\n",
    "        cv2.imwrite('current_face.jpg', face_roi)\n",
    "        try:\n",
    "            for user_image in os.listdir(\"users/\"):\n",
    "                user_name = os.path.splitext(user_image)[0]\n",
    "                user_image_path = os.path.join(\"users\", user_image)\n",
    "                result = DeepFace.verify('current_face.jpg', user_image_path, enforce_detection=False)\n",
    "                if result['verified']:\n",
    "                    authenticated = True\n",
    "                    authenticated_user = user_name\n",
    "                    speak(f\"Welcome {authenticated_user}\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Authentication Error: {e}\")\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "def camera_loop():\n",
    "    global prev_x, prev_y, last_ocr_time\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        root.after(10, camera_loop)\n",
    "        return\n",
    "\n",
    "    frame_raw = frame.copy()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(100, 100))\n",
    "\n",
    "    if not authenticated:\n",
    "        authenticate_user(frame, faces)\n",
    "    else:\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks and not ocr_mode:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                landmarks = [coord for lm in hand_landmarks.landmark for coord in (lm.x, lm.y)]\n",
    "                h, w, _ = frame.shape\n",
    "                index_tip = hand_landmarks.landmark[8]\n",
    "                cx, cy = int(index_tip.x * w), int(index_tip.y * h)\n",
    "\n",
    "                if draw_mode:\n",
    "                    if prev_x is not None:\n",
    "                        cv2.line(canvas, (prev_x, prev_y), (cx, cy), draw_color, 5)\n",
    "                    prev_x, prev_y = cx, cy\n",
    "                else:\n",
    "                    prev_x, prev_y = None, None\n",
    "                    try:\n",
    "                        prediction = clf.predict(np.array(landmarks).reshape(1, -1))\n",
    "                        gesture = label_mapping[prediction[0]]\n",
    "                        handle_gesture(gesture)\n",
    "                        cv2.putText(frame, f'Gesture: {gesture}', (10, 30),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Prediction Error]: {e}\")\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if ocr_mode:\n",
    "            current_time = time.time()\n",
    "            if current_time - last_ocr_time > ocr_cooldown:\n",
    "                ocr_input = frame_raw.copy()\n",
    "                frame = cv2.resize(ocr_input, (640, 480))\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "                _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "                custom_config = r'--oem 3 --psm 6'\n",
    "                details = pytesseract.image_to_data(thresh, config=custom_config, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "                detected_text = \"\"\n",
    "                for i in range(len(details['text'])):\n",
    "                    text = details['text'][i]\n",
    "                    confidence = int(details['conf'][i])\n",
    "                    if text.strip() != \"\" and confidence > 90:\n",
    "                        detected_text += text + \" \"\n",
    "                        speak(detected_text)\n",
    "                cv2.putText(frame, detected_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        frame = cv2.addWeighted(frame, 1, canvas, 0.5, 0)\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_label.imgtk = imgtk\n",
    "    video_label.configure(image=imgtk)\n",
    "\n",
    "    root.after(10, camera_loop)\n",
    "\n",
    "# Buttons\n",
    "button_frame = ttk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "row1 = ttk.Frame(button_frame)\n",
    "row1.pack(pady=3)\n",
    "row2 = ttk.Frame(button_frame)\n",
    "row2.pack(pady=3)\n",
    "\n",
    "# Gesture & OCR controls\n",
    "ttk.Button(row1, text=\"Toggle Draw Mode\", command=toggle_draw_mode).pack(side=tk.LEFT, padx=5)\n",
    "ttk.Button(row1, text=\"Toggle OCR Mode\", command=toggle_ocr_mode).pack(side=tk.LEFT, padx=5)\n",
    "ttk.Button(row1, text=\"Clear Canvas\", command=clear_canvas).pack(side=tk.LEFT, padx=5)\n",
    "ttk.Button(row1, text=\"Toggle Voice Commands\", command=toggle_voice_commands).pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Drawing colors\n",
    "color_buttons = [\n",
    "    (\"Red\", (0, 0, 255)),\n",
    "    (\"Green\", (0, 255, 0)),\n",
    "    (\"Blue\", (255, 0, 0)),\n",
    "    (\"Yellow\", (0, 255, 255)),\n",
    "    (\"Black\", (0, 0, 0))\n",
    "]\n",
    "for label, color in color_buttons:\n",
    "    ttk.Button(row2, text=label, command=lambda c=color: set_color(c)).pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Start app\n",
    "camera_loop()\n",
    "root.mainloop()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10999b14",
   "metadata": {},
   "source": [
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ead96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pyautogui\n",
    "import time\n",
    "import pyttsx3\n",
    "from PIL import Image, ImageTk\n",
    "import pytesseract\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "# Set path to Tesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Load gesture recognition model\n",
    "with open(r\"D:\\nada mossad\\e-just\\4th grade\\second term\\computer vision\\Project\\gestures\\gesture_rf_model.pkl\", 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'palm - play/pause',\n",
    "    1: 'like - volume up',\n",
    "    2: 'dislike - volume down',\n",
    "    3: 'peace - scroll up',\n",
    "    4: 'four - scroll down'\n",
    "}\n",
    "\n",
    "# Load camera calibration data\n",
    "with open(r\"D:\\nada mossad\\e-just\\4th grade\\second term\\computer vision\\Project\\gestures\\camera_calibration_data.pkl\", 'rb') as f:\n",
    "    calibration_data = pickle.load(f)\n",
    "camera_matrix = calibration_data['camera_matrix']\n",
    "dist_coeffs = calibration_data['distortion_coefficients']\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.6)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Voice engine\n",
    "engine = pyttsx3.init()\n",
    "def speak(text):\n",
    "    try:\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Globals\n",
    "cap = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "authenticated = False\n",
    "authenticated_user = \"\"\n",
    "\n",
    "canvas = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "draw_mode = False\n",
    "ocr_mode = False\n",
    "draw_color = (0, 255, 0)\n",
    "prev_x, prev_y = None, None\n",
    "prev_gesture = None\n",
    "last_action_time = 0\n",
    "cooldown = 1.5\n",
    "ocr_cooldown = 2\n",
    "last_ocr_time = 0\n",
    "\n",
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Smart Vision Assistant\")\n",
    "root.configure(bg=\"#2e2e2e\")\n",
    "\n",
    "style = ttk.Style()\n",
    "style.theme_use(\"clam\")\n",
    "style.configure(\"TButton\", font=(\"Segoe UI\", 10), padding=6, relief=\"flat\", background=\"#3a3a3a\", foreground=\"white\")\n",
    "style.configure(\"TFrame\", background=\"#2e2e2e\")\n",
    "\n",
    "video_label = tk.Label(root)\n",
    "video_label.pack(pady=10)\n",
    "\n",
    "# Functions\n",
    "def toggle_draw_mode():\n",
    "    global draw_mode\n",
    "    draw_mode = not draw_mode\n",
    "    print(\"Draw mode:\", draw_mode)\n",
    "\n",
    "def toggle_ocr_mode():\n",
    "    global ocr_mode\n",
    "    ocr_mode = not ocr_mode\n",
    "    print(\"OCR mode:\", ocr_mode)\n",
    "\n",
    "def clear_canvas():\n",
    "    global canvas\n",
    "    canvas.fill(0)\n",
    "    print(\"Canvas cleared\")\n",
    "\n",
    "def set_color(color):\n",
    "    global draw_color\n",
    "    draw_color = color\n",
    "    print(\"Color changed:\", color)\n",
    "\n",
    "def handle_gesture(gesture):\n",
    "    global prev_gesture, last_action_time\n",
    "    current_time = time.time()\n",
    "    if gesture != prev_gesture or current_time - last_action_time > cooldown:\n",
    "        prev_gesture = gesture\n",
    "        last_action_time = current_time\n",
    "        speak(gesture)\n",
    "\n",
    "        if gesture == 'palm - play/pause':\n",
    "            pyautogui.press('playpause')\n",
    "        elif gesture == 'like - volume up':\n",
    "            pyautogui.press('volumeup')\n",
    "        elif gesture == 'dislike - volume down':\n",
    "            pyautogui.press('volumedown')\n",
    "        elif gesture == 'peace - scroll up':\n",
    "            pyautogui.scroll(300)\n",
    "        elif gesture == 'four - scroll down':\n",
    "            pyautogui.scroll(-300)\n",
    "\n",
    "def authenticate_user(frame, faces):\n",
    "    global authenticated, authenticated_user\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "        if face_roi.size == 0:\n",
    "            continue\n",
    "        cv2.imwrite('current_face.jpg', face_roi)\n",
    "        try:\n",
    "            for user_image in os.listdir(\"users/\"):\n",
    "                user_name = os.path.splitext(user_image)[0]\n",
    "                user_image_path = os.path.join(\"users\", user_image)\n",
    "                result = DeepFace.verify('current_face.jpg', user_image_path, enforce_detection=False)\n",
    "                if result['verified']:\n",
    "                    authenticated = True\n",
    "                    authenticated_user = user_name\n",
    "                    speak(f\"Welcome {authenticated_user}\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Authentication Error: {e}\")\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "def camera_loop():\n",
    "    global prev_x, prev_y, last_ocr_time\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        root.after(10, camera_loop)\n",
    "        return\n",
    "\n",
    "    frame_raw = frame.copy()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Undistort the frame using the camera calibration data\n",
    "    frame = cv2.undistort(frame, camera_matrix, dist_coeffs)\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(100, 100))\n",
    "\n",
    "    if not authenticated:\n",
    "        authenticate_user(frame, faces)\n",
    "    else:\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks and not ocr_mode:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                landmarks = [coord for lm in hand_landmarks.landmark for coord in (lm.x, lm.y)]\n",
    "                h, w, _ = frame.shape\n",
    "                index_tip = hand_landmarks.landmark[8]\n",
    "                cx, cy = int(index_tip.x * w), int(index_tip.y * h)\n",
    "\n",
    "                if draw_mode:\n",
    "                    if prev_x is not None:\n",
    "                        cv2.line(canvas, (prev_x, prev_y), (cx, cy), draw_color, 5)\n",
    "                    prev_x, prev_y = cx, cy\n",
    "                else:\n",
    "                    prev_x, prev_y = None, None\n",
    "                    try:\n",
    "                        prediction = clf.predict(np.array(landmarks).reshape(1, -1))\n",
    "                        gesture = label_mapping[prediction[0]]\n",
    "                        handle_gesture(gesture)\n",
    "                        cv2.putText(frame, f'Gesture: {gesture}', (10, 30),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[Prediction Error]: {e}\")\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if ocr_mode:\n",
    "            current_time = time.time()\n",
    "            if current_time - last_ocr_time > ocr_cooldown:\n",
    "                ocr_input = frame_raw.copy()\n",
    "                frame = cv2.resize(ocr_input, (640, 480))\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "                _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "                custom_config = r'--oem 3 --psm 6'\n",
    "                details = pytesseract.image_to_data(thresh, config=custom_config, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "                detected_text = \"\"\n",
    "                for i in range(len(details['text'])):\n",
    "                    text = details['text'][i]\n",
    "                    confidence = int(details['conf'][i])\n",
    "                    if text.strip() != \"\" and confidence > 90:\n",
    "                        detected_text += text + \" \"\n",
    "                        speak(detected_text)\n",
    "                cv2.putText(frame, detected_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        frame = cv2.addWeighted(frame, 1, canvas, 0.5, 0)\n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_label.imgtk = imgtk\n",
    "    video_label.configure(image=imgtk)\n",
    "\n",
    "    root.after(10, camera_loop)\n",
    "\n",
    "# Buttons\n",
    "button_frame = ttk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "row1 = ttk.Frame(button_frame)\n",
    "row1.pack(pady=3)\n",
    "row2 = ttk.Frame(button_frame)\n",
    "row2.pack(pady=3)\n",
    "\n",
    "ttk.Button(row1, text=\"Toggle Draw Mode\", command=toggle_draw_mode).pack(side=tk.LEFT, padx=5)\n",
    "ttk.Button(row1, text=\"Toggle OCR Mode\", command=toggle_ocr_mode).pack(side=tk.LEFT, padx=5)\n",
    "ttk.Button(row1, text=\"Clear Canvas\", command=clear_canvas).pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "color_buttons = [\n",
    "    (\"Red\", (0, 0, 255)),\n",
    "    (\"Green\", (0, 255, 0)),\n",
    "    (\"Blue\", (255, 0, 0)),\n",
    "    (\"Yellow\", (0, 255, 255)),\n",
    "    (\"Black\", (0, 0, 0))\n",
    "]\n",
    "for label, color in color_buttons:\n",
    "    ttk.Button(row2, text=label, command=lambda c=color: set_color(c)).pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Run\n",
    "camera_loop()\n",
    "root.mainloop()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d827278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
